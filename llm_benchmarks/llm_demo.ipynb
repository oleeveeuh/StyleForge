{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "if ffn_results:\n    print(\"\\n\" + \"=\"*70)\n    print(\"FFN BENCHMARK RESULTS\")\n    print(\"=\"*70)\n    print(f\"\\n{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10} {'Memory Saved':<15}\")\n    print(\"-\"*70)\n    \n    for r in ffn_results:\n        # Calculate memory saved\n        mem_saved_mb = (r['seq_len'] * config.intermediate_size * 4) / (1024**2)\n        print(f\"{r['seq_len']:<10} {r['pytorch_mean']:<15.2f} {r['custom_mean']:<15.2f} {r['speedup']:<10.2f}x {mem_saved_mb:<15.1f} MB\")\n    \n    # Calculate average speedup\n    avg_speedup = np.mean([r['speedup'] for r in ffn_results])\n    print(f\"\\nAverage speedup: {avg_speedup:.2f}x\")\n    \n    # Save FFN results\n    results_dir = Path('results')\n    with open(results_dir / 'ffn_benchmark.json', 'w') as f:\n        json.dump(ffn_results, f, indent=2)\n    print(f\"\\nFFN results saved to: {results_dir / 'ffn_benchmark.json'}\")\nelse:\n    print(\"\\nNo FFN results to display (GPU may not be available)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and check GPU availability."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# PROMPT 4: Documentation Updates\n\n*(Coming soon - will add for final documentation)*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary\n\nThis notebook demonstrates:\n\n1. **PROMPT 1**:\n   - Llama-2-7B configuration loading\n   - Dummy input generation for attention and FFN\n   - Memory usage estimation\n\n2. **PROMPT 2**:\n   - Custom attention kernel loading\n   - Numerical correctness validation\n   - Performance benchmarking across sequence lengths\n   - Memory savings analysis\n\n3. **PROMPT 3**:\n   - Custom FFN kernel loading\n   - Numerical correctness validation\n   - Performance benchmarking with memory savings\n\n4. **PROMPT 4**: *(to be added)*\n\n### Key Results\n\n- **Attention**: O(N) memory vs O(N²) for standard attention, online softmax\n- **FFN**: Eliminates intermediate tensor allocations, fused GELU activation\n- Combined speedup from optimized kernels\n\n### Next Steps\n\n- Run this notebook on a GPU with CUDA to get actual benchmark numbers\n- Compare with Flash Attention 2 baseline\n- Add documentation updates (PROMPT 4)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modules\n",
    "from configs.llama2_7b import LLAMA2_7B, get_config\n",
    "from models.utils import (\n",
    "    create_dummy_attention_inputs,\n",
    "    create_dummy_ffn_inputs,\n",
    "    estimate_memory_usage,\n",
    "    validate_attention_output,\n",
    "    get_gpu_memory_info,\n",
    "    print_gpu_info\n",
    ")\n",
    "from scripts.benchmark_harness import BenchmarkHarness\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Information\n",
    "\n",
    "Check CUDA availability and display GPU specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nCUDA is available!\")\n",
    "    print_gpu_info()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "else:\n",
    "    print(\"\\nCUDA is NOT available. Please run this notebook on a GPU machine.\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PROMPT 1: LLM Infrastructure Setup\n",
    "\n",
    "## Model Configuration\n",
    "\n",
    "Load Llama-2-7B configuration for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Llama-2-7B configuration\n",
    "config = LLAMA2_7B\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Llama-2-7B Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hidden size:           {config.hidden_size}\")\n",
    "print(f\"Num layers:            {config.num_hidden_layers}\")\n",
    "print(f\"Num attention heads:   {config.num_attention_heads}\")\n",
    "print(f\"Num KV heads:          {config.num_key_value_heads}\")\n",
    "print(f\"Head dimension:        {config.head_dim}\")\n",
    "print(f\"Intermediate size:     {config.intermediate_size}\")\n",
    "print(f\"Max position embeddings: {config.max_position_embeddings}\")\n",
    "print(f\"Vocab size:            {config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dummy Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention input creation\n",
    "seq_len = 512\n",
    "\n",
    "print(f\"\\nTesting dummy input generation for seq_len={seq_len}...\\n\")\n",
    "\n",
    "Q, K, V = create_dummy_attention_inputs(\n",
    "    batch_size=1,\n",
    "    seq_len=seq_len,\n",
    "    num_heads=config.num_attention_heads,\n",
    "    head_dim=config.head_dim,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(\"Attention inputs created:\")\n",
    "print(f\"  Q shape: {Q.shape}\")\n",
    "print(f\"  K shape: {K.shape}\")\n",
    "print(f\"  V shape: {V.shape}\")\n",
    "print(f\"  Device:  {Q.device}\")\n",
    "print(f\"  Q mean:  {Q.mean().item():.4f}\")\n",
    "print(f\"  Q std:   {Q.std().item():.4f}\")\n",
    "\n",
    "# Test FFN input creation\n",
    "x, w1, w2 = create_dummy_ffn_inputs(\n",
    "    batch_size=1,\n",
    "    seq_len=seq_len,\n",
    "    hidden_dim=config.hidden_size,\n",
    "    intermediate_dim=config.intermediate_size,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(\"\\nFFN inputs created:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  W1 shape:    {w1.shape}\")\n",
    "print(f\"  W2 shape:    {w2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate memory usage across sequence lengths\n",
    "print(\"\\n=\"*70)\n",
    "print(\"Memory Usage Estimates (per layer, float32)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Seq Len':<10} {'Attention (MB)':<18} {'Attn Scores (MB)':<18} {'FFN (MB)':<12} {'Total (MB)':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for seq_len in [256, 512, 1024, 2048, 4096]:\n",
    "    mem = estimate_memory_usage(\n",
    "        seq_len=seq_len,\n",
    "        hidden_dim=config.hidden_size,\n",
    "        num_heads=config.num_attention_heads,\n",
    "        intermediate_dim=config.intermediate_size,\n",
    "    )\n",
    "    print(f\"{seq_len:<10} {mem['attention_mb']:<18.2f} {mem['attention_scores_mb']:<18.2f} {mem['ffn_mb']:<12.2f} {mem['total_mb']:<12.2f}\")\n",
    "\n",
    "print(\"\\nNote: Custom attention kernel avoids storing O(N²) attention matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PROMPT 2: Attention Kernel Benchmarking\n",
    "\n",
    "## Load Custom Attention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom attention\n",
    "from models.custom_attention import CustomMultiHeadAttention, create_pytorch_baseline_attention\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Loading Attention Kernels\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    custom_attn = CustomMultiHeadAttention(\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_heads=config.num_attention_heads,\n",
    "    ).cuda().eval()\n",
    "    print(\"\\nCustom attention kernel: LOADED\")\n",
    "    print(f\"  Using custom kernel: {custom_attn.use_custom_kernel}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCustom attention kernel error: {e}\")\n",
    "    custom_attn = None\n",
    "\n",
    "try:\n",
    "    pytorch_attn = create_pytorch_baseline_attention(\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_heads=config.num_attention_heads,\n",
    "    ).cuda().eval()\n",
    "    print(\"\\nPyTorch baseline: LOADED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nPyTorch baseline error: {e}\")\n",
    "    pytorch_attn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Correctness Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate correctness on a smaller test case\n",
    "print(\"\\n=\"*70)\n",
    "print(\"Numerical Correctness Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if custom_attn and pytorch_attn:\n",
    "    test_seq_len = 256\n",
    "    hidden_states = torch.randn(\n",
    "        1, test_seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        custom_output = custom_attn(hidden_states)\n",
    "        pytorch_output, _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "    \n",
    "    is_close, max_error, mean_error = validate_attention_output(\n",
    "        custom_output, pytorch_output, rtol=1e-3, atol=1e-4\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSequence length: {test_seq_len}\")\n",
    "    print(f\"Output shape: {custom_output.shape}\")\n",
    "    print(f\"Max error:  {max_error:.2e}\")\n",
    "    print(f\"Mean error: {mean_error:.2e}\")\n",
    "    print(f\"\\nResult: {'PASS - Outputs match within tolerance' if is_close else 'FAIL - Outputs differ'}\")\n",
    "else:\n",
    "    print(\"\\nSkipping: models not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "Benchmark across multiple sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def benchmark_single_seq_len(seq_len, warmup=10, iters=50):\n",
    "    \"\"\"Benchmark attention at a single sequence length\"\"\"\n",
    "    if not custom_attn or not pytorch_attn:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nBenchmarking seq_len={seq_len}...\")\n",
    "    \n",
    "    hidden_states = torch.randn(\n",
    "        1, seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        with torch.no_grad():\n",
    "            _ = custom_attn(hidden_states)\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(iters):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    # Benchmark Custom\n",
    "    custom_times = []\n",
    "    for _ in range(iters):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = custom_attn(hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        custom_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    return {\n",
    "        'seq_len': seq_len,\n",
    "        'pytorch_mean': np.mean(pytorch_times),\n",
    "        'pytorch_std': np.std(pytorch_times),\n",
    "        'custom_mean': np.mean(custom_times),\n",
    "        'custom_std': np.std(custom_times),\n",
    "        'speedup': np.mean(pytorch_times) / np.mean(custom_times),\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Attention Performance Benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096]\n",
    "results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    try:\n",
    "        result = benchmark_single_seq_len(seq_len)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BENCHMARK RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['seq_len']:<10} {r['pytorch_mean']:<15.2f} {r['custom_mean']:<15.2f} {r['speedup']:<10.2f}x\")\n",
    "    \n",
    "    # Calculate average speedup\n",
    "    avg_speedup = np.mean([r['speedup'] for r in results])\n",
    "    print(f\"\\nAverage speedup: {avg_speedup:.2f}x\")\n",
    "    \n",
    "    # Memory savings\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Memory Savings (Attention Matrix Avoided)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Seq Len':<10} {'Attention Matrix (MB)':<25} {'Saved':<10}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    for seq_len in [512, 1024, 2048, 4096]:\n",
    "        attn_matrix_mb = (config.num_attention_heads * seq_len * seq_len * 4) / (1024**2)\n",
    "        saved_gb = attn_matrix_mb / 1024\n",
    "        print(f\"{seq_len:<10} {attn_matrix_mb:<25.2f} ~{saved_gb:<10.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nNo results to display (GPU may not be available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results\n",
    "if results:\n",
    "    results_dir = Path('results')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save detailed results\n",
    "    with open(results_dir / 'attention_benchmark.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {results_dir / 'attention_benchmark.json'}\")\n",
    "    \n",
    "    # Display saved file\n",
    "    print(\"\\nSaved results:\")\n",
    "    print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Import custom FFN\nfrom models.custom_ffn import CustomFFN, PyTorchFFN, count_ffn_operations\n\nprint(\"=\"*70)\nprint(\"Loading FFN Kernels\")\nprint(\"=\"*70)\n\ntry:\n    custom_ffn = CustomFFN(\n        hidden_size=config.hidden_size,\n        intermediate_size=config.intermediate_size,\n    ).cuda().eval()\n    print(\"\\nCustom FFN kernel: LOADED\")\n    print(f\"  Using custom kernel: {custom_ffn.use_custom_kernel}\")\nexcept Exception as e:\n    print(f\"\\nCustom FFN kernel error: {e}\")\n    custom_ffn = None\n\ntry:\n    pytorch_ffn = PyTorchFFN(\n        hidden_size=config.hidden_size,\n        intermediate_size=config.intermediate_size,\n    ).cuda().eval()\n    print(\"\\nPyTorch baseline: LOADED\")\nexcept Exception as e:\n    print(f\"\\nPyTorch baseline error: {e}\")\n    pytorch_ffn = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Validate FFN correctness\nprint(\"\\n\" + \"=\"*70)\nprint(\"FFN Numerical Correctness Test\")\nprint(\"=\"*70)\n\nif custom_ffn and pytorch_ffn:\n    test_seq_len = 256\n    hidden_states = torch.randn(\n        1, test_seq_len, config.hidden_size,\n        dtype=torch.float32, device='cuda'\n    )\n    \n    with torch.no_grad():\n        custom_output = custom_ffn(hidden_states)\n        pytorch_output = pytorch_ffn(hidden_states)\n    \n    is_close, max_error, mean_error = validate_attention_output(\n        custom_output, pytorch_output, rtol=1e-3, atol=1e-4\n    )\n    \n    print(f\"\\nSequence length: {test_seq_len}\")\n    print(f\"Output shape: {custom_output.shape}\")\n    print(f\"Max error:  {max_error:.2e}\")\n    print(f\"Mean error: {mean_error:.2e}\")\n    print(f\"\\nResult: {'PASS - Outputs match within tolerance' if is_close else 'FAIL - Outputs differ'}\")\nelse:\n    print(\"\\nSkipping: models not loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def benchmark_ffn_single_seq_len(seq_len, warmup=10, iters=50):\n    \"\"\"Benchmark FFN at a single sequence length\"\"\"\n    if not custom_ffn or not pytorch_ffn:\n        return None\n    \n    print(f\"\\nBenchmarking FFN seq_len={seq_len}...\")\n    \n    hidden_states = torch.randn(\n        1, seq_len, config.hidden_size,\n        dtype=torch.float32, device='cuda'\n    )\n    \n    # Warmup\n    for _ in range(warmup):\n        with torch.no_grad():\n            _ = custom_ffn(hidden_states)\n            _ = pytorch_ffn(hidden_states)\n    torch.cuda.synchronize()\n    \n    # Benchmark PyTorch\n    pytorch_times = []\n    for _ in range(iters):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        with torch.no_grad():\n            _ = pytorch_ffn(hidden_states)\n        end.record()\n        torch.cuda.synchronize()\n        pytorch_times.append(start.elapsed_time(end))\n    \n    # Benchmark Custom\n    custom_times = []\n    for _ in range(iters):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n        with torch.no_grad():\n            _ = custom_ffn(hidden_states)\n        end.record()\n        torch.cuda.synchronize()\n        custom_times.append(start.elapsed_time(end))\n    \n    return {\n        'seq_len': seq_len,\n        'pytorch_mean': np.mean(pytorch_times),\n        'pytorch_std': np.std(pytorch_times),\n        'custom_mean': np.mean(custom_times),\n        'custom_std': np.std(custom_times),\n        'speedup': np.mean(pytorch_times) / np.mean(custom_times),\n    }\n\n# Run FFN benchmarks\nprint(\"\\n\" + \"=\"*70)\nprint(\"FFN Performance Benchmark\")\nprint(\"=\"*70)\n\nffn_seq_lengths = [512, 1024, 2048, 4096]\nffn_results = []\n\nfor seq_len in ffn_seq_lengths:\n    try:\n        result = benchmark_ffn_single_seq_len(seq_len)\n        if result:\n            ffn_results.append(result)\n    except RuntimeError as e:\n        print(f\"  Error: {e}\")\n        continue"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
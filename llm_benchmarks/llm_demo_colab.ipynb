{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerOpt: LLM Kernel Benchmarks (Colab Version - V4)\n",
    "\n",
    "This notebook benchmarks the **FIXED V4** attention kernel.\n",
    "\n",
    "## What Was Fixed\n",
    "\n",
    "- **V3 Bug**: Recomputed K/V for every key position (1000x slowdown!)\n",
    "- **V3 Bug**: Only 1 of 8 warps contributed (wrong answers)\n",
    "- **V4 Fix**: Pre-computed Q,K,V + proper multi-warp reduction\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Select GPU runtime**: Runtime > Change runtime type > T4 GPU\n",
    "2. **Upload kernel files**: `attention_v4.cu`, `attention_v4_wrapper.py`, `utils.py`\n",
    "3. **Run all cells** sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Kernel Files\n",
    "\n",
    "**Required files** (upload to `kernels/` directory):\n",
    "- `utils.py`\n",
    "- `attention_v4.cu` (the FIXED kernel)\n",
    "- `attention_v4_wrapper.py`\n",
    "- `ffn.cu`\n",
    "- `ffn_wrapper.py`\n",
    "\n",
    "Click the folder icon (ðŸ“) on the left, then upload files to `kernels/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if kernel files exist\n",
    "import os\n",
    "\n",
    "# Create kernels directory if needed\n",
    "!mkdir -p kernels\n",
    "\n",
    "required_files = [\n",
    "    'kernels/utils.py',\n",
    "    'kernels/attention_v4.cu',  # V4 - FIXED\n",
    "    'kernels/attention_v4_wrapper.py',\n",
    "    'kernels/ffn.cu',\n",
    "    'kernels/ffn_wrapper.py'\n",
    "]\n",
    "\n",
    "print(\"Checking for required kernel files...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "all_exist = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = \" found\" if exists else \" NOT FOUND\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n All required kernel files found!\")\n",
    "else:\n",
    "    print(\"\\n WARNING: Some kernel files are missing.\")\n",
    "    print(\" Please upload them to the kernels/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n CUDA is available!\")\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\" PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\" Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\" Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\" Multi-Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\\n CUDA is NOT available!\")\n",
    "    print(\" Please enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install torch ninja numpy -q\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile V4 Attention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile V4 attention kernel\n",
    "print(\"Compiling Attention V4 kernel (FIXED version)...\")\n",
    "try:\n",
    "    from kernels.attention_v4_wrapper import get_attention_v4_module\n",
    "    \n",
    "    # Compile and test\n",
    "    module = get_attention_v4_module()\n",
    "    print(\" Attention V4 kernel compiled successfully!\")\n",
    "    attention_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error compiling attention kernel: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    attention_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Correctness Test\n",
    "\n",
    "Test that V4 produces correct results before benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if not attention_available:\n",
    "    print(\"SKIP: V4 kernel not available\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Correctness Test: V4 vs PyTorch\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Small test\n",
    "    B, S, E, H = 1, 128, 512, 4\n",
    "    head_dim = E // H\n",
    "    scale = 1.0 / (head_dim ** 0.5)\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(B, S, E, device='cuda')\n",
    "    \n",
    "    # PyTorch reference\n",
    "    qkv = x.unfold(1, E, E).reshape(B, S, 3, H, head_dim).permute(2,0,3,1,4)\n",
    "    Q_pt, K_pt, V_pt = qkv[0], qkv[1], qkv[2]\n",
    "    \n",
    "    scores_pt = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n",
    "    attn_pt = F.softmax(scores_pt, dim=-1)\n",
    "    output_pt = attn_pt @ V_pt\n",
    "    \n",
    "    # V4 custom\n",
    "    with torch.no_grad():\n",
    "        output_v4 = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n",
    "    \n",
    "    # Compare\n",
    "    error = (output_v4 - output_pt).abs().max().item()\n",
    "    print(f\"\\nMax error: {error:.2e}\")\n",
    "    \n",
    "    if error < 1e-4:\n",
    "        print(\" CORRECTNESS: PASS\")\n",
    "    else:\n",
    "        print(\" CORRECTNESS: FAIL - error too large\")\n",
    "        print(f\"  V4 range: [{output_v4.min().item():.4f}, {output_v4.max().item():.4f}]\")\n",
    "        print(f\"  PyTorch range: [{output_pt.min().item():.4f}, {output_pt.max().item():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark V4 vs PyTorch\n",
    "\n",
    "Compare performance on realistic LLM configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "if not attention_available:\n",
    "    print(\"SKIP: V4 kernel not available\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Attention Kernel Benchmark (V4 FIXED)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test configurations\n",
    "    configs = [\n",
    "        {\"name\": \"Small (B=1, S=128, E=512, H=4)\", \"B\": 1, \"S\": 128, \"E\": 512, \"H\": 4},\n",
    "        {\"name\": \"Medium (B=1, S=256, E=512, H=4)\", \"B\": 1, \"S\": 256, \"E\": 512, \"H\": 4},\n",
    "        {\"name\": \"Large (B=1, S=256, E=1024, H=8)\", \"B\": 1, \"S\": 256, \"E\": 1024, \"H\": 8},\n",
    "        {\"name\": \"Llama-like (B=1, S=512, E=2048, H=16)\", \"B\": 1, \"S\": 512, \"E\": 2048, \"H\": 16},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        B, S, E, H = cfg[\"B\"], cfg[\"S\"], cfg[\"E\"], cfg[\"H\"]\n",
    "        head_dim = E // H\n",
    "        scale = 1.0 / (head_dim ** 0.5)\n",
    "        \n",
    "        print(f\"\\n{cfg['name']} (S={S}, E={E}, H={H})\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Create input\n",
    "        x = torch.randn(B, S, E, device='cuda')\n",
    "        \n",
    "        # Compute QKV manually (both use same)\n",
    "        W = torch.randn(3*E, E, device='cuda')\n",
    "        qkv = x @ W.T  # [B, S, 3*E]\n",
    "        qkv = qkv.reshape(B, S, 3, H, head_dim).permute(2, 0, 3, 1, 4)\n",
    "        Q_pt, K_pt, V_pt = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark V4\n",
    "        v4_times = []\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            v4_times.append(start.elapsed_time(end))\n",
    "        \n",
    "        v4_mean = np.mean(v4_times)\n",
    "        v4_std = np.std(v4_times)\n",
    "        print(f\"  V4 Custom:  {v4_mean:.3f} Â± {v4_std:.3f} ms\")\n",
    "        \n",
    "        # Benchmark PyTorch (manual QKV + SDPA)\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                scores = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n",
    "                _ = F.softmax(scores, dim=-1) @ V_pt\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        pt_times = []\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
"            end.record()",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                scores = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n",
    "                _ = F.softmax(scores, dim=-1) @ V_pt\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            pt_times.append(start.elapsed_time(end))\n",
    "        \n",
    "        pt_mean = np.mean(pt_times)\n",
    "        pt_std = np.std(pt_times)\n",
    "        print(f\"  PyTorch:    {pt_mean:.3f} Â± {pt_std:.3f} ms\")\n",
    "        \n",
    "        speedup = pt_mean / v4_mean\n",
    "        print(f\"  Speedup:    {speedup:.2f}x\")\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": cfg['name'],\n",
    "            \"pytorch_ms\": pt_mean,\n",
    "            \"v4_ms\": v4_mean,\n",
    "            \"speedup\": speedup\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention_available and 'results' in locals() and results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'Config':<30} {'PyTorch (ms)':<15} {'V4 (ms)':<12} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<30} {r['pytorch_ms']:<15.3f} {r['v4_ms']:<12.3f} {r['speedup']:<10.2f}x\")\n",
    "    \n",
    "    avg_speedup = np.mean([r['speedup'] for r in results])\n",
    "    print(f\"\\nAverage speedup: {avg_speedup:.2f}x\")\n",
    "    \n",
    "    if avg_speedup >= 1.0:\n",
    "        print(\"\\n V4 is FASTER than PyTorch!\")\n",
    "    elif avg_speedup >= 0.5:\n",
    "        print(\"\\n V4 is competitive (within 2x)\")\n",
    "    else:\n",
    "        print(\"\\n V4 is still slower - needs more optimization\")\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if attention_available and 'results' in locals() and results:\n",
    "    results_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'gpu': torch.cuda.get_device_name(0),\n",
    "        'cuda_version': torch.version.cuda,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    filename = 'benchmark_v4_results.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    # Download file (Colab only)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(filename)\n",
    "        print(\"\\nResults downloaded!\")\n",
    "    except:\n",
    "        print(\"\\nFile saved locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

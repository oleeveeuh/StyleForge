{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerOpt: LLM Kernel Benchmarks (Colab Version)\n",
    "\n",
    "This notebook benchmarks custom CUDA kernels for LLM inference on Google Colab.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Upload your kernels** as a zip file using the file upload cell below\n",
    "2. **Select GPU runtime**: Runtime > Change runtime type > T4 GPU\n",
    "3. **Run all cells** sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Check\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n CUDA is available!\")\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\" PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Get GPU properties\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\" Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\" Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\" Multi-Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\\n CUDA is NOT available!\")\n",
    "    print(\" Please enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Kernels\n",
    "\n",
    "**Option A: Upload a zip file** containing your `kernels/` directory\n",
    "\n",
    "**Option B: Upload individual files** - upload these files one at a time:\n",
    "- `kernels/utils.py`\n",
    "- `kernels/attention_v3.cu`\n",
    "- `kernels/attention_v3_wrapper.py`\n",
    "- `kernels/ffn.cu`\n",
    "- `kernels/ffn_wrapper.py`\n",
    "- `kernels/__init__.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kernels directory structure\n",
    "!mkdir -p kernels\n",
    "\n",
    "print(\"Please upload your kernel files using the file upload button below.\")\n",
    "print(\"\\nUpload these files:\")\n",
    "print(\"  1. utils.py (from kernels/)\")\n",
    "print(\"  2. attention_v3.cu\")\n",
    "print(\"  3. attention_v3_wrapper.py\")\n",
    "print(\"  4. ffn.cu\")\n",
    "print(\"  5. ffn_wrapper.py\")\n",
    "print(\"  6. __init__.py\")\n",
    "print(\"\\nOr upload a zip file and run the unzip cell below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you uploaded a zip file, uncomment and run this\n",
    "# !unzip -o kernels.zip -d .\n",
    "\n",
    "# Check if files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'kernels/utils.py',\n",
    "    'kernels/attention_v3.cu', \n",
    "    'kernels/attention_v3_wrapper.py',\n",
    "    'kernels/ffn.cu',\n",
    "    'kernels/ffn_wrapper.py'\n",
    "]\n",
    "\n",
    "print(\"Checking for required files...\")\n",
    "all_exist = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = \" found\" if exists else \" NOT FOUND\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n All required files found!\")\n",
    "else:\n",
    "    print(\"\\n WARNING: Some files are missing. Please upload them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install torch ninja numpy -q\n",
    "\n",
    "# Verify PyTorch has CUDA\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile Kernels\n",
    "\n",
    "This will compile your CUDA kernels on first import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile attention kernel\n",
    "print(\"Compiling Attention V3 kernel...\")\n",
    "try:\n",
    "    from kernels.attention_v3_wrapper import FusedAttentionV3\n",
    "    print(\" Attention V3 kernel compiled successfully!\")\n",
    "    attention_available = True\n",
    "except Exception as e:\n",
    "    print(f\" Error compiling attention kernel: {e}\")\n",
    "    attention_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile FFN kernel\n",
    "print(\"Compiling FFN kernel...\")\n",
    "try:\n",
    "    from kernels.ffn_wrapper import FusedFFN\n",
    "    print(\" FFN kernel compiled successfully!\")\n",
    "    ffn_available = True\n",
    "except Exception as e:\n",
    "    print(f\" Error compiling FFN kernel: {e}\")\n",
    "    ffn_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Llama-2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Llama2Config:\n",
    "    \"\"\"Llama-2-7B configuration\"\"\"\n",
    "    hidden_size: int = 4096\n",
    "    num_hidden_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 32\n",
    "    intermediate_size: int = 11008\n",
    "    max_position_embeddings: int = 4096\n",
    "    vocab_size: int = 32000\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        return self.hidden_size // self.num_attention_heads\n",
    "\n",
    "config = Llama2Config()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Llama-2-7B Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hidden size:           {config.hidden_size}\")\n",
    "print(f\"Num layers:            {config.num_hidden_layers}\")\n",
    "print(f\"Num attention heads:   {config.num_attention_heads}\")\n",
    "print(f\"Head dimension:        {config.head_dim}\")\n",
    "print(f\"Intermediate size:     {config.intermediate_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark Attention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Attention Kernel Benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration for Colab T4 (smaller sizes for faster testing)\n",
    "seq_lengths = [256, 512, 1024, 2048]\n",
    "batch_size = 1\n",
    "\n",
    "attention_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nBenchmarking seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create custom attention\n",
    "    if attention_available:\n",
    "        custom_attn = FusedAttentionV3(\n",
    "            embed_dim=config.hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "        ).cuda().eval()\n",
    "    \n",
    "    # Create PyTorch baseline\n",
    "    pytorch_attn = nn.MultiheadAttention(\n",
    "        embed_dim=config.hidden_size,\n",
    "        num_heads=config.num_attention_heads,\n",
    "        batch_first=True,\n",
    "    ).cuda().eval()\n",
    "    \n",
    "    # Create input\n",
    "    hidden_states = torch.randn(\n",
    "        batch_size, seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if attention_available:\n",
    "                _ = custom_attn(hidden_states)\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(50):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    # Benchmark Custom\n",
    "    custom_times = []\n",
    "    if attention_available:\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = custom_attn(hidden_states)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            custom_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    pytorch_mean = np.mean(pytorch_times)\n",
    "    pytorch_std = np.std(pytorch_times)\n",
    "    print(f\"  PyTorch: {pytorch_mean:.3f} ± {pytorch_std:.3f} ms\")\n",
    "    \n",
    "    if attention_available:\n",
    "        custom_mean = np.mean(custom_times)\n",
    "        custom_std = np.std(custom_times)\n",
    "        speedup = pytorch_mean / custom_mean\n",
    "        print(f\"  Custom:  {custom_mean:.3f} ± {custom_std:.3f} ms\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        attention_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': custom_mean,\n",
    "            'speedup': speedup\n",
    "        })\n",
    "    else:\n",
    "        attention_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': None,\n",
    "            'speedup': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Benchmark FFN Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FFN Kernel Benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ffn_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nBenchmarking FFN seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create custom FFN\n",
    "    if ffn_available:\n",
    "        custom_ffn = FusedFFN(\n",
    "            embed_dim=config.hidden_size,\n",
    "            ffn_dim=config.intermediate_size,\n",
    "        ).cuda().eval()\n",
    "    \n",
    "    # Create PyTorch baseline\n",
    "    pytorch_ffn = nn.Sequential(\n",
    "        nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    ).cuda().eval()\n",
    "    \n",
    "    # Create input\n",
    "    hidden_states = torch.randn(\n",
    "        batch_size, seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if ffn_available:\n",
    "                _ = custom_ffn(hidden_states)\n",
    "            _ = pytorch_ffn(hidden_states)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(50):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = pytorch_ffn(hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    # Benchmark Custom\n",
    "    custom_times = []\n",
    "    if ffn_available:\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = custom_ffn(hidden_states)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            custom_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    pytorch_mean = np.mean(pytorch_times)\n",
    "    pytorch_std = np.std(pytorch_times)\n",
    "    print(f\"  PyTorch: {pytorch_mean:.3f} ± {pytorch_std:.3f} ms\")\n",
    "    \n",
    "    if ffn_available:\n",
    "        custom_mean = np.mean(custom_times)\n",
    "        custom_std = np.std(custom_times)\n",
    "        speedup = pytorch_mean / custom_mean\n",
    "        print(f\"  Custom:  {custom_mean:.3f} ± {custom_std:.3f} ms\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        memory_saved = (seq_len * config.intermediate_size * 4) / (1024**2)\n",
    "        print(f\"  Memory saved: {memory_saved:.1f} MB\")\n",
    "        \n",
    "        ffn_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': custom_mean,\n",
    "            'speedup': speedup,\n",
    "            'memory_saved_mb': memory_saved\n",
    "        })\n",
    "    else:\n",
    "        ffn_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': None,\n",
    "            'speedup': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ATTENTION KERNEL RESULTS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for r in attention_results:\n",
    "    seq = r['seq_len']\n",
    "    pt = r['pytorch_ms']\n",
    "    cust = r['custom_ms'] if r['custom_ms'] else 'N/A'\n",
    "    sp = f\"{r['speedup']:.2f}x\" if r['speedup'] else 'N/A'\n",
    "    print(f\"{seq:<10} {pt:<15.3f} {str(cust):<15} {sp:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FFN KERNEL RESULTS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10} {'Mem Saved':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in ffn_results:\n",
    "    seq = r['seq_len']\n",
    "    pt = r['pytorch_ms']\n",
    "    cust = r['custom_ms'] if r['custom_ms'] else 'N/A'\n",
    "    sp = f\"{r['speedup']:.2f}x\" if r['speedup'] else 'N/A'\n",
    "    mem = f\"{r.get('memory_saved_mb', 0):.1f} MB\" if r.get('memory_saved_mb') else 'N/A'\n",
    "    print(f\"{seq:<10} {pt:<15.3f} {str(cust):<15} {sp:<10} {mem:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Results\n",
    "\n",
    "Run this cell to save and download your benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect all results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'gpu': torch.cuda.get_device_name(0),\n",
    "    'cuda_version': torch.version.cuda,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'config': {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'num_heads': config.num_attention_heads,\n",
    "        'intermediate_size': config.intermediate_size,\n",
    "    },\n",
    "    'attention_results': attention_results,\n",
    "    'ffn_results': ffn_results\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "filename = 'benchmark_results.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Download file\n",
    "from google.colab import files\n",
    "files.download(filename)\n",
    "\n",
    "print(\"\\nResults downloaded! You can now update your README with these numbers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

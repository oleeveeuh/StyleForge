{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerOpt: LLM Kernel Benchmarks (Colab Version)\n",
    "\n",
    "This notebook benchmarks custom CUDA kernels for LLM inference on Google Colab.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Select GPU runtime**: Runtime > Change runtime type > T4 GPU\n",
    "2. **Run all cells** sequentially\n",
    "\n",
    "The notebook will automatically clone your GitHub repository and compile the kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "# Replace 'YOUR_USERNAME' and 'REPO_NAME' with your actual GitHub details\n",
    "\n",
    "import os\n",
    "\n",
    "# TODO: Update this with your actual GitHub URL\n",
    "GITHUB_URL = \"https://github.com/YOUR_USERNAME/StyleForge.git\"\n",
    "\n",
    "# For now, using the current directory structure if cloned\n",
    "print(\"=\"*70)\n",
    "print(\"Repository Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if we're running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"\\nRunning in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"\\nNot in Colab - using local files\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nCloning repository...\")\n",
    "    print(f\"\\nNOTE: Update GITHUB_URL with your repo:\")\n",
    "    print(f\"  Current: {GITHUB_URL}\")\n",
    "    print(f\"\\nThen uncomment the line below:\")\n",
    "    print(f\"  # !git clone {GITHUB_URL}\")\n",
    "    \n",
    "    # Uncomment and modify the line below with your actual repo\n",
    "    # !git clone {GITHUB_URL}\n",
    "    \n",
    "    # For now, create the structure for manual upload\n",
    "    !mkdir -p StyleForge/kernels\n",
    "    %cd StyleForge\n",
    "    print(\"\\nPlease upload your kernel files to StyleForge/kernels/ directory\")\n",
    "else:\n",
    "    print(\"\\nUsing local directory structure\")\n",
    "    # We're already in the project directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Kernel Files (Colab Only)\n",
    "\n",
    "**If running in Colab**, upload these files to the `kernels/` directory:\n",
    "- `utils.py`\n",
    "- `attention_v3.cu`\n",
    "- `attention_v3_wrapper.py`\n",
    "- `ffn.cu`\n",
    "- `ffn_wrapper.py`\n",
    "\n",
    "Click the folder icon (ðŸ“) on the left, then upload files to `StyleForge/kernels/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if kernel files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'kernels/utils.py',\n",
    "    'kernels/attention_v3.cu', \n",
    "    'kernels/attention_v3_wrapper.py',\n",
    "    'kernels/ffn.cu',\n",
    "    'kernels/ffn_wrapper.py'\n",
    "]\n",
    "\n",
    "print(\"Checking for required kernel files...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "all_exist = True\n",
    "for f in required_files:\n",
    "    exists = os.path.exists(f)\n",
    "    status = \" found\" if exists else \" NOT FOUND\"\n",
    "    print(f\"  {f}: {status}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "if all_exist:\n",
    "    print(\"\\n All required kernel files found!\")\n",
    "else:\n",
    "    print(\"\\n WARNING: Some kernel files are missing.\")\n",
    "    print(\" Please upload them to the kernels/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n CUDA is available!\")\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\" PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\" Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\" Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\" Multi-Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\\n CUDA is NOT available!\")\n",
    "    print(\" Please enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install torch ninja numpy -q\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compile Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile attention kernel\n",
    "print(\"Compiling Attention V3 kernel...\")\n",
    "try:\n",
    "    from kernels.attention_v3_wrapper import FusedAttentionV3\n",
    "    print(\" Attention V3 kernel compiled successfully!\")\n",
    "    attention_available = True\n",
    "except Exception as e:\n",
    "    print(f\" Error compiling attention kernel: {e}\")\n",
    "    attention_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile FFN kernel\n",
    "print(\"Compiling FFN kernel...\")\n",
    "try:\n",
    "    from kernels.ffn_wrapper import FusedFFN\n",
    "    print(\" FFN kernel compiled successfully!\")\n",
    "    ffn_available = True\n",
    "except Exception as e:\n",
    "    print(f\" Error compiling FFN kernel: {e}\")\n",
    "    ffn_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Llama-2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Llama2Config:\n",
    "    \"\"\"Llama-2-7B configuration\"\"\"\n",
    "    hidden_size: int = 4096\n",
    "    num_hidden_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 32\n",
    "    intermediate_size: int = 11008\n",
    "    max_position_embeddings: int = 4096\n",
    "    vocab_size: int = 32000\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self) -> int:\n",
    "        return self.hidden_size // self.num_attention_heads\n",
    "\n",
    "config = Llama2Config()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Llama-2-7B Configuration\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hidden size:           {config.hidden_size}\")\n",
    "print(f\"Num layers:            {config.num_hidden_layers}\")\n",
    "print(f\"Num attention heads:   {config.num_attention_heads}\")\n",
    "print(f\"Head dimension:        {config.head_dim}\")\n",
    "print(f\"Intermediate size:     {config.intermediate_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Benchmark Attention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Attention Kernel Benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sequence lengths to test (smaller for Colab T4)\n",
    "seq_lengths = [256, 512, 1024, 2048]\n",
    "batch_size = 1\n",
    "\n",
    "attention_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nBenchmarking seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create custom attention\n",
    "    if attention_available:\n",
    "        custom_attn = FusedAttentionV3(\n",
    "            embed_dim=config.hidden_size,\n",
    "            num_heads=config.num_attention_heads,\n",
    "        ).cuda().eval()\n",
    "    \n",
    "    # Create PyTorch baseline\n",
    "    pytorch_attn = nn.MultiheadAttention(\n",
    "        embed_dim=config.hidden_size,\n",
    "        num_heads=config.num_attention_heads,\n",
    "        batch_first=True,\n",
    "    ).cuda().eval()\n",
    "    \n",
    "    # Create input\n",
    "    hidden_states = torch.randn(\n",
    "        batch_size, seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if attention_available:\n",
    "                _ = custom_attn(hidden_states)\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(50):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = pytorch_attn(hidden_states, hidden_states, hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    # Benchmark Custom\n",
    "    custom_times = []\n",
    "    if attention_available:\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = custom_attn(hidden_states)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            custom_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    pytorch_mean = np.mean(pytorch_times)\n",
    "    pytorch_std = np.std(pytorch_times)\n",
    "    print(f\"  PyTorch: {pytorch_mean:.3f} Â± {pytorch_std:.3f} ms\")\n",
    "    \n",
    "    if attention_available:\n",
    "        custom_mean = np.mean(custom_times)\n",
    "        custom_std = np.std(custom_times)\n",
    "        speedup = pytorch_mean / custom_mean\n",
    "        print(f\"  Custom:  {custom_mean:.3f} Â± {custom_std:.3f} ms\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        attention_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': custom_mean,\n",
    "            'speedup': speedup\n",
    "        })\n",
    "    else:\n",
    "        attention_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': None,\n",
    "            'speedup': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Benchmark FFN Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FFN Kernel Benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ffn_results = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    print(f\"\\nBenchmarking FFN seq_len={seq_len}...\")\n",
    "    \n",
    "    # Create custom FFN\n",
    "    if ffn_available:\n",
    "        custom_ffn = FusedFFN(\n",
    "            embed_dim=config.hidden_size,\n",
    "            ffn_dim=config.intermediate_size,\n",
    "        ).cuda().eval()\n",
    "    \n",
    "    # Create PyTorch baseline\n",
    "    pytorch_ffn = nn.Sequential(\n",
    "        nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "    ).cuda().eval()\n",
    "    \n",
    "    # Create input\n",
    "    hidden_states = torch.randn(\n",
    "        batch_size, seq_len, config.hidden_size,\n",
    "        dtype=torch.float32, device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            if ffn_available:\n",
    "                _ = custom_ffn(hidden_states)\n",
    "            _ = pytorch_ffn(hidden_states)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    pytorch_times = []\n",
    "    for _ in range(50):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with torch.no_grad():\n",
    "            _ = pytorch_ffn(hidden_states)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    # Benchmark Custom\n",
    "    custom_times = []\n",
    "    if ffn_available:\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = custom_ffn(hidden_states)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            custom_times.append(start.elapsed_time(end))\n",
    "    \n",
    "    pytorch_mean = np.mean(pytorch_times)\n",
    "    pytorch_std = np.std(pytorch_times)\n",
    "    print(f\"  PyTorch: {pytorch_mean:.3f} Â± {pytorch_std:.3f} ms\")\n",
    "    \n",
    "    if ffn_available:\n",
    "        custom_mean = np.mean(custom_times)\n",
    "        custom_std = np.std(custom_times)\n",
    "        speedup = pytorch_mean / custom_mean\n",
    "        print(f\"  Custom:  {custom_mean:.3f} Â± {custom_std:.3f} ms\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        memory_saved = (seq_len * config.intermediate_size * 4) / (1024**2)\n",
    "        print(f\"  Memory saved: {memory_saved:.1f} MB\")\n",
    "        \n",
    "        ffn_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': custom_mean,\n",
    "            'speedup': speedup,\n",
    "            'memory_saved_mb': memory_saved\n",
    "        })\n",
    "    else:\n",
    "        ffn_results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_mean,\n",
    "            'custom_ms': None,\n",
    "            'speedup': None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"ATTENTION KERNEL RESULTS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for r in attention_results:\n",
    "    seq = r['seq_len']\n",
    "    pt = r['pytorch_ms']\n",
    "    cust = r['custom_ms'] if r['custom_ms'] else 'N/A'\n",
    "    sp = f\"{r['speedup']:.2f}x\" if r['speedup'] else 'N/A'\n",
    "    print(f\"{seq:<10} {pt:<15.3f} {str(cust):<15} {sp:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FFN KERNEL RESULTS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Seq Len':<10} {'PyTorch (ms)':<15} {'Custom (ms)':<15} {'Speedup':<10} {'Mem Saved':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in ffn_results:\n",
    "    seq = r['seq_len']\n",
    "    pt = r['pytorch_ms']\n",
    "    cust = r['custom_ms'] if r['custom_ms'] else 'N/A'\n",
    "    sp = f\"{r['speedup']:.2f}x\" if r['speedup'] else 'N/A'\n",
    "    mem = f\"{r.get('memory_saved_mb', 0):.1f} MB\" if r.get('memory_saved_mb') else 'N/A'\n",
    "    print(f\"{seq:<10} {pt:<15.3f} {str(cust):<15} {sp:<10} {mem:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Results\n",
    "\n",
    "Run this cell to save and download your benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect all results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'gpu': torch.cuda.get_device_name(0),\n",
    "    'cuda_version': torch.version.cuda,\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'config': {\n",
    "        'hidden_size': config.hidden_size,\n",
    "        'num_heads': config.num_attention_heads,\n",
    "        'intermediate_size': config.intermediate_size,\n",
    "    },\n",
    "    'attention_results': attention_results,\n",
    "    'ffn_results': ffn_results\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "filename = 'benchmark_results.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Download file (Colab only)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    print(\"\\nResults downloaded!\")\n",
    "except:\n",
    "    print(\"\\nFile saved locally. Download location:\")\n",
    "    print(f\"  {os.path.abspath(filename)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

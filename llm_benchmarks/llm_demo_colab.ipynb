{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerOpt: LLM Kernel Benchmarks (Colab Version - V4)\n",
    "\n",
    "This notebook benchmarks the **FIXED V4** attention kernel.\n",
    "\n",
    "## What Was Fixed\n",
    "\n",
    "- **V3 Bug**: Recomputed K/V for every key position (1000x slowdown!)\n",
    "- **V3 Bug**: Only 1 of 8 warps contributed (wrong answers)\n",
    "- **V4 Fix**: Pre-computed Q,K,V + proper multi-warp reduction\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Select GPU runtime**: Runtime > Change runtime type > T4 GPU\n",
    "2. **Upload kernel files**: `attention_v4.cu`, `attention_v4_wrapper.py`, `utils.py`\n",
    "3. **Run all cells** sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 0: Clone Repository (OPTIONAL)\n\n**Option A - Clone from GitHub** (recommended):\n\n\n**Option B - Upload files manually**:\n- Click the folder icon (\ud83d\udcc1) on the left\n- Upload , ,  to "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option A: Clone from GitHub (run this cell)\nimport os\n\nif not os.path.exists('kernels'):\n    print(\"Cloning StyleForge repository...\")\n    !git clone https://github.com/olivialiau/StyleForge.git\n    %cd StyleForge\n    print(\"Repository cloned successfully!\")\nelse:\n    print(\"Repository already exists.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Verify Kernel Files\n\nCheck that all required files are present."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if kernel files exist\nimport os\nimport sys\n\n# Detect where we are and set paths correctly\nif os.path.exists('kernels'):\n    # We're in the repo root\n    kernel_path = 'kernels'\n    print(\"Working from repo root\")\nelif os.path.exists('../kernels'):\n    # We're in llm_benchmarks subdir\n    kernel_path = '../kernels'\n    sys.path.insert(0, os.path.abspath('..'))\n    print(\"Working from llm_benchmarks/ subdir\")\nelse:\n    # Try to find kernels dir\n    for p in ['.', '..', '../..']:\n        if os.path.exists(os.path.join(p, 'kernels')):\n            kernel_path = os.path.join(p, 'kernels')\n            sys.path.insert(0, os.path.abspath(p))\n            print(f\"Found kernels at {kernel_path}\")\n            break\n    else:\n        kernel_path = 'kernels'\n        print(\"Could not find kernels directory\")\n\nrequired_files = [\n    f'{kernel_path}/utils.py',\n    f'{kernel_path}/attention_v4.cu',\n    f'{kernel_path}/attention_v4_wrapper.py',\n    f'{kernel_path}/ffn.cu',\n    f'{kernel_path}/ffn_wrapper.py'\n]\n\nprint(\"\\nChecking for required kernel files...\")\nprint(\"-\"*50)\n\nall_exist = True\nfor f in required_files:\n    exists = os.path.exists(f)\n    status = \"\u2713 found\" if exists else \"\u2717 NOT FOUND\"\n    print(f\"  {f}: {status}\")\n    if not exists:\n        all_exist = False\n\nif all_exist:\n    print(\"\\n\u2713 All required kernel files found!\")\nelse:\n    print(\"\\n\u26a0 WARNING: Some kernel files are missing.\")\n    print(\"  Make sure you cloned the repo or uploaded files.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU Information\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n CUDA is available!\")\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\" PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\" Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\" Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\" Multi-Processors: {props.multi_processor_count}\")\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"\\n CUDA is NOT available!\")\n",
    "    print(\" Please enable GPU: Runtime > Change runtime type > T4 GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "!pip install torch ninja numpy -q\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compile V4 Attention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compile V4 attention kernel\nimport sys\nimport os\n\n# Set up path to import from kernels/\nif os.path.exists('kernels'):\n    sys.path.insert(0, '.')\nelif os.path.exists('../kernels'):\n    sys.path.insert(0, os.path.abspath('..'))\n\nprint(\"Compiling Attention V4 kernel (FIXED version)...\")\ntry:\n    from kernels.attention_v4_wrapper import get_attention_v4_module\n    \n    # Compile and test\n    module = get_attention_v4_module()\n    print(\"\u2713 Attention V4 kernel compiled successfully!\")\n    attention_available = True\n    \nexcept Exception as e:\n    print(f\"\u2717 Error compiling attention kernel: {e}\")\n    import traceback\n    traceback.print_exc()\n    attention_available = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Correctness Test\n",
    "\n",
    "Test that V4 produces correct results before benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\n\nif not attention_available:\n    print(\"SKIP: V4 kernel not available\")\nelse:\n    print(\"=\"*70)\n    print(\"Correctness Test: V4 vs PyTorch\")\n    print(\"=\"*70)\n    \n    # Small test\n    B, S, E, H = 1, 128, 512, 4\n    head_dim = E // H\n    scale = 1.0 / (head_dim ** 0.5)\n    \n    # Create input\n    x = torch.randn(B, S, E, device='cuda')\n    \n    # PyTorch reference - compute QKV properly\n    W_qkv = torch.randn(3 * E, E, device='cuda')\n    qkv = x @ W_qkv.T  # [B, S, 3*E]\n    qkv = qkv.reshape(B, S, 3, H, head_dim).permute(2, 0, 3, 1, 4)\n    Q_pt, K_pt, V_pt = qkv[0], qkv[1], qkv[2]\n    \n    scores_pt = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n    attn_pt = F.softmax(scores_pt, dim=-1)\n    output_pt = attn_pt @ V_pt\n    \n    # V4 custom\n    with torch.no_grad():\n        output_v4 = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n    \n    # Compare\n    error = (output_v4 - output_pt).abs().max().item()\n    print(f\"\\nMax error: {error:.2e}\")\n    \n    if error < 1e-4:\n        print(\"\u2713 CORRECTNESS: PASS\")\n    else:\n        print(\"\u2717 CORRECTNESS: FAIL - error too large\")\n        print(f\"  V4 range: [{output_v4.min().item():.4f}, {output_v4.max().item():.4f}]\")\n        print(f\"  PyTorch range: [{output_pt.min().item():.4f}, {output_pt.max().item():.4f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Benchmark V4 vs PyTorch\n",
    "\n",
    "Compare performance on realistic LLM configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "if not attention_available:\n",
    "    print(\"SKIP: V4 kernel not available\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Attention Kernel Benchmark (V4 FIXED)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test configurations\n",
    "    configs = [\n",
    "        {\"name\": \"Small (B=1, S=128, E=512, H=4)\", \"B\": 1, \"S\": 128, \"E\": 512, \"H\": 4},\n",
    "        {\"name\": \"Medium (B=1, S=256, E=512, H=4)\", \"B\": 1, \"S\": 256, \"E\": 512, \"H\": 4},\n",
    "        {\"name\": \"Large (B=1, S=256, E=1024, H=8)\", \"B\": 1, \"S\": 256, \"E\": 1024, \"H\": 8},\n",
    "        {\"name\": \"Llama-like (B=1, S=512, E=2048, H=16)\", \"B\": 1, \"S\": 512, \"E\": 2048, \"H\": 16},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for cfg in configs:\n",
    "        B, S, E, H = cfg[\"B\"], cfg[\"S\"], cfg[\"E\"], cfg[\"H\"]\n",
    "        head_dim = E // H\n",
    "        scale = 1.0 / (head_dim ** 0.5)\n",
    "        \n",
    "        print(f\"\\n{cfg['name']} (S={S}, E={E}, H={H})\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Create input\n",
    "        x = torch.randn(B, S, E, device='cuda')\n",
    "        \n",
    "        # Compute QKV manually (both use same)\n",
    "        W = torch.randn(3*E, E, device='cuda')\n",
    "        qkv = x @ W.T  # [B, S, 3*E]\n",
    "        qkv = qkv.reshape(B, S, 3, H, head_dim).permute(2, 0, 3, 1, 4)\n",
    "        Q_pt, K_pt, V_pt = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark V4\n",
    "        v4_times = []\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                _ = get_attention_v4_module().fused_attention_v4(Q_pt, K_pt, V_pt, scale)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            v4_times.append(start.elapsed_time(end))\n",
    "        \n",
    "        v4_mean = np.mean(v4_times)\n",
    "        v4_std = np.std(v4_times)\n",
    "        print(f\"  V4 Custom:  {v4_mean:.3f} \u00b1 {v4_std:.3f} ms\")\n",
    "        \n",
    "        # Benchmark PyTorch (manual QKV + SDPA)\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                scores = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n",
    "                _ = F.softmax(scores, dim=-1) @ V_pt\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        pt_times = []\n",
    "        for _ in range(50):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            end.record()",
    "            start.record()\n",
    "            with torch.no_grad():\n",
    "                scores = (Q_pt @ K_pt.transpose(-2, -1)) * scale\n",
    "                _ = F.softmax(scores, dim=-1) @ V_pt\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            pt_times.append(start.elapsed_time(end))\n",
    "        \n",
    "        pt_mean = np.mean(pt_times)\n",
    "        pt_std = np.std(pt_times)\n",
    "        print(f\"  PyTorch:    {pt_mean:.3f} \u00b1 {pt_std:.3f} ms\")\n",
    "        \n",
    "        speedup = pt_mean / v4_mean\n",
    "        print(f\"  Speedup:    {speedup:.2f}x\")\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": cfg['name'],\n",
    "            \"pytorch_ms\": pt_mean,\n",
    "            \"v4_ms\": v4_mean,\n",
    "            \"speedup\": speedup\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if attention_available and 'results' in locals() and results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(f\"{'Config':<30} {'PyTorch (ms)':<15} {'V4 (ms)':<12} {'Speedup':<10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<30} {r['pytorch_ms']:<15.3f} {r['v4_ms']:<12.3f} {r['speedup']:<10.2f}x\")\n",
    "    \n",
    "    avg_speedup = np.mean([r['speedup'] for r in results])\n",
    "    print(f\"\\nAverage speedup: {avg_speedup:.2f}x\")\n",
    "    \n",
    "    if avg_speedup >= 1.0:\n",
    "        print(\"\\n V4 is FASTER than PyTorch!\")\n",
    "    elif avg_speedup >= 0.5:\n",
    "        print(\"\\n V4 is competitive (within 2x)\")\n",
    "    else:\n",
    "        print(\"\\n V4 is still slower - needs more optimization\")\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if attention_available and 'results' in locals() and results:\n",
    "    results_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'gpu': torch.cuda.get_device_name(0),\n",
    "        'cuda_version': torch.version.cuda,\n",
    "        'pytorch_version': torch.__version__,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    filename = 'benchmark_v4_results.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    # Download file (Colab only)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(filename)\n",
    "        print(\"\\nResults downloaded!\")\n",
    "    except:\n",
    "        print(\"\\nFile saved locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
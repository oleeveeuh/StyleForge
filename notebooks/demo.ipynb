{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleForge - Real-Time Neural Style Transfer with CUDA Kernels\n",
    "\n",
    "This notebook demonstrates the StyleForge system with optimized CUDA kernels for real-time neural style transfer.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Fused Multi-Head Attention**: 4-8x faster than PyTorch\n",
    "- **Fused FFN**: 3-5x speedup for feed-forward layers\n",
    "- **Fused Instance Norm**: 2-4x faster normalization\n",
    "- **Fused Conv+InstanceNorm+ReLU**: 5-8x speedup for residual blocks\n",
    "- **Comprehensive Benchmarking**: Automated profiling and reporting\n",
    "- **Nsight Compute Integration**: Deep GPU performance analysis\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- CUDA 11.0+ GPU with Compute Capability 7.0+\n",
    "- PyTorch 1.10+ with CUDA support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Repository\n",
    "\n",
    "Clone and navigate to the StyleForge directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    REPO_DIR = \"/content/StyleForge\"\n",
    "    print(\"üìå Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    REPO_DIR = None\n",
    "    print(\"üìå Not running in Google Colab\")\n",
    "\n",
    "# Navigate to repository\n",
    "if Path.cwd().name == \"StyleForge\":\n",
    "    print(\"Already in StyleForge directory\")\n",
    "elif (Path.cwd().parent / \"StyleForge\").exists():\n",
    "    os.chdir(\"../StyleForge\")\n",
    "    print(\"Changed to parent StyleForge directory\")\n",
    "elif IN_COLAB and not Path(REPO_DIR).exists():\n",
    "    # Clone repository\n",
    "    print(f\"Cloning StyleForge repository to {REPO_DIR}...\")\n",
    "    !git clone https://github.com/oleeveeuh/StyleForge.git {REPO_DIR}\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "print(f\"\\nWorking directory: {Path.cwd()}\")\n",
    "print(f\"Repository exists: {(Path.cwd() / 'kernels').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"StyleForge - CUDA Kernel Demo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"\\n‚úÖ CUDA available!\")\n",
    "    print(f\"   GPU: {props.name}\")\n",
    "    print(f\"   Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   Total Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Streaming MPs: {props.multi_processor_count}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"\\n‚ö†Ô∏è CUDA not available - using CPU\")\n",
    "\n",
    "# Add project to path\n",
    "if str(Path.cwd()) not in sys.path:\n",
    "    sys.path.insert(0, str(Path.cwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load StyleForge Kernels\n",
    "\n",
    "The kernels will be JIT-compiled on first use. This may take 30-60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading StyleForge CUDA Kernels...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import all kernels\n",
    "from kernels import (\n",
    "    FusedAttention,\n",
    "    FusedFFN,\n",
    "    FusedInstanceNorm2d,\n",
    "    FusedConvInstanceNormReLU,\n",
    "    ResidualBlock,\n",
    "    benchmark_conv_fusion_vs_pytorch,\n",
    "    run_conv_fusion_benchmark,\n",
    ")\n",
    "\n",
    "# Import benchmarking framework\n",
    "from benchmarking import (\n",
    "    BenchmarkFramework,\n",
    "    BenchmarkConfig,\n",
    "    BenchmarkReport,\n",
    "    BenchmarkVisualizer,\n",
    "    HAS_MATPLOTLIB,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All kernels imported successfully!\")\n",
    "print(\"\\nAvailable kernels:\")\n",
    "print(\"  - FusedAttention (4-8x speedup)\")\n",
    "print(\"  - FusedFFN (3-5x speedup)\")\n",
    "print(\"  - FusedInstanceNorm2d (2-4x speedup)\")\n",
    "print(\"  - FusedConvInstanceNormReLU (5-8x speedup)\")\n",
    "print(\"  - ResidualBlock (uses fused kernels)\")\n",
    "print(\"\\nBenchmarking framework:\")\n",
    "print(\"  - BenchmarkFramework (automated timing)\")\n",
    "print(\"  - BenchmarkReport (MD/JSON/HTML/CSV)\")\n",
    "print(f\"  - BenchmarkVisualizer (matplotlib: {HAS_MATPLOTLIB})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Kernel Demonstration\n",
    "\n",
    "Test each fused kernel with correctness validation and speedup measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Quick Kernel Demonstration\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test configurations\n",
    "test_configs = [\n",
    "    (\"Small\", 1, 64, 64, 64),\n",
    "    (\"Medium\", 1, 128, 128, 128),\n",
    "]\n",
    "\n",
    "for name, batch, channels, h, w in test_configs:\n",
    "    print(f\"\\n{name}: [{batch}, {channels}, {h}, {w}]\")\n",
    "    x = torch.randn(batch, channels, h, w, device=device)\n",
    "\n",
    "    # Test FusedInstanceNorm2d\n",
    "    try:\n",
    "        norm = FusedInstanceNorm2d(channels).to(device).eval()\n",
    "        with torch.no_grad():\n",
    "            out = norm(x)\n",
    "        print(f\"  ‚úÖ FusedInstanceNorm2d: {out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå FusedInstanceNorm2d: {e}\")\n",
    "\n",
    "    # Test FusedConvInstanceNormReLU\n",
    "    try:\n",
    "        conv = FusedConvInstanceNormReLU(channels, channels, 3).to(device).eval()\n",
    "        with torch.no_grad():\n",
    "            out = conv(x)\n",
    "        print(f\"  ‚úÖ FusedConv+IN+ReLU: {out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå FusedConv+IN+ReLU: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All kernels working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Benchmarks\n",
    "\n",
    "Use the BenchmarkFramework to automatically compare kernels against PyTorch baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Automated Benchmarking with BenchmarkFramework\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create framework\n",
    "framework = BenchmarkFramework(use_cuda_events=True)\n",
    "\n",
    "# Define test configurations\n",
    "configs = [\n",
    "    BenchmarkConfig(\"64√ó64\", 1, 64, 64, 64, iterations=50),\n",
    "    BenchmarkConfig(\"128√ó128\", 1, 128, 128, 128, iterations=50),\n",
    "    BenchmarkConfig(\"256√ó256\", 1, 64, 256, 256, iterations=30),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    # Create input\n",
    "    x = framework.create_input_tensor(config)\n",
    "\n",
    "    # PyTorch baseline\n",
    "    pytorch_norm = nn.InstanceNorm2d(config.channels, affine=True).to(device).eval()\n",
    "\n",
    "    # Fused kernel\n",
    "    fused_norm = FusedInstanceNorm2d(config.channels, affine=True).to(device).eval()\n",
    "\n",
    "    # Copy weights\n",
    "    with torch.no_grad():\n",
    "        fused_norm.gamma.copy_(pytorch_norm.weight)\n",
    "        fused_norm.beta.copy_(pytorch_norm.bias)\n",
    "\n",
    "    # Compare\n",
    "    result = framework.compare(\n",
    "        baseline_func=pytorch_norm,\n",
    "        optimized_func=fused_norm,\n",
    "        config=config,\n",
    "        input_tensor=x,\n",
    "        validate=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    if result is not None:\n",
    "        results.append(result.to_dict())\n",
    "\n",
    "# Print summary\n",
    "framework.print_summary()\n",
    "\n",
    "# Save results\n",
    "framework.save_results('benchmark_results/demo_results.json')\n",
    "print(\"\\n‚úÖ Results saved to benchmark_results/demo_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Professional Reports\n",
    "\n",
    "Create markdown, JSON, HTML, and CSV reports from benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Generating Benchmark Reports\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create report generator\n",
    "report = BenchmarkReport(\n",
    "    title=\"StyleForge CUDA Kernel Performance Report\",\n",
    "    subtitle=\"Fused InstanceNorm2d Benchmark Results\"\n",
    ")\n",
    "\n",
    "# Generate all formats\n",
    "output_dir = Path('benchmark_results/demo_reports')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report.generate_all_formats(results, str(output_dir))\n",
    "\n",
    "print(f\"\\n‚úÖ Reports saved to: {output_dir}/\")\n",
    "print(\"   - report.md  (Markdown)\")\n",
    "print(\"   - report.json (JSON)\")\n",
    "print(\"   - report.html (HTML)\")\n",
    "print(\"   - report.csv (CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Performance (Optional)\n",
    "\n",
    "Generate charts if matplotlib is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_MATPLOTLIB:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Generating Performance Charts\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    visualizer = BenchmarkVisualizer()\n",
    "    charts_dir = output_dir / 'charts'\n",
    "    visualizer.generate_all_charts(results, str(charts_dir))\n",
    "\n",
    "    print(f\"\\n‚úÖ Charts saved to: {charts_dir}/\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è matplotlib not installed - skipping charts\")\n",
    "    print(\"   Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conv+InstanceNorm+ReLU Benchmark\n",
    "\n",
    "Run the comprehensive fused convolution benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Fused Conv+InstanceNorm+ReLU Benchmark\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run the comprehensive benchmark\n",
    "from kernels.conv_fusion_wrapper import run_comprehensive_benchmark\n",
    "\n",
    "# This will run benchmarks across multiple configurations\n",
    "# Note: This may take a minute or two\n",
    "conv_results = run_comprehensive_benchmark()\n",
    "\n",
    "print(\"\\n‚úÖ Conv fusion benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Style Transfer Model Demo\n",
    "\n",
    "Now let's use these kernels in an actual style transfer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer_net import TransformerNet, AVAILABLE_STYLES\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Fast Style Transfer Model\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nAvailable styles: {', '.join(AVAILABLE_STYLES)}\")\n",
    "\n",
    "# Create model\n",
    "style_model = TransformerNet(num_residual_blocks=5).to(device)\n",
    "style_model.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in style_model.parameters())\n",
    "print(f\"\\nModel parameters: {total_params:,}\")\n",
    "print(\"‚úÖ Model loaded\")\n",
    "\n",
    "# Test the model\n",
    "x = torch.randn(1, 3, 256, 256, device=device)\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        _ = style_model(x)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        output = style_model(x)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "avg_ms = np.mean(times)\n",
    "fps = 1000 / avg_ms\n",
    "\n",
    "print(f\"\\nStyle Transfer Performance (256x256):\")\n",
    "print(f\"  Latency: {avg_ms:.2f} ms\")\n",
    "print(f\"  FPS: {fps:.2f}\")\n",
    "print(f\"  Real-time: {'‚úÖ YES' if fps >= 30 else '‚ùå NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Image Style Transfer\n",
    "\n",
    "Upload an image and apply style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    from io import BytesIO\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torchvision import transforms\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Image Upload & Style Transfer\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nüìÅ Upload an image:\\n\")\n",
    "\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if uploaded:\n",
    "        for filename in uploaded.keys():\n",
    "            print(f\"\\nProcessing {filename}...\")\n",
    "\n",
    "            # Load image\n",
    "            img = Image.open(BytesIO(uploaded[filename])).convert('RGB')\n",
    "            original_size = img.size\n",
    "\n",
    "            # Resize for processing\n",
    "            PROCESSING_SIZE = 512\n",
    "            aspect = img.size[0] / img.size[1]\n",
    "            if aspect > 1:\n",
    "                new_size = (PROCESSING_SIZE, int(PROCESSING_SIZE / aspect))\n",
    "            else:\n",
    "                new_size = (int(PROCESSING_SIZE * aspect), PROCESSING_SIZE)\n",
    "            img_resized = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "            # Convert to tensor\n",
    "            transform = transforms.Compose([transforms.ToTensor()])\n",
    "            input_tensor = transform(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "            # Apply style transfer\n",
    "            with torch.no_grad():\n",
    "                start = time.perf_counter()\n",
    "                output_tensor = style_model(input_tensor)\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "\n",
    "            # Convert back\n",
    "            to_pil = transforms.ToPILImage()\n",
    "            output_img = to_pil(output_tensor.squeeze(0).clamp(0, 1))\n",
    "            output_img = output_img.resize(original_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "            # Display\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title('Original')\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(output_img)\n",
    "            axes[1].set_title(f'Stylized ({elapsed_ms:.1f} ms)')\n",
    "            axes[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Save and download\n",
    "            result_filename = f'stylized_{filename}'\n",
    "            output_img.save(result_filename, quality=95)\n",
    "            print(f\"‚úÖ Saved: {result_filename}\")\n",
    "            files.download(result_filename)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nNote: Image upload works in Google Colab.\")\n",
    "    print(\"For local Jupyter, use:\")\n",
    "    print(\"  from PIL import Image\")\n",
    "    print(\"  img = Image.open('path/to/image.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Nsight Compute Profiling\n",
    "\n",
    "Profile kernels with NVIDIA Nsight Compute for deep GPU analysis.\n",
    "\n",
    "**Note:** This requires Nsight Compute to be installed locally.\n",
    "Colab does not support ncu profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if ncu is available\n",
    "ncu_available = shutil.which('ncu') is not None\n",
    "\n",
    "if ncu_available:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Nsight Compute Profiling\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n‚úÖ ncu found - profiling available!\")\n",
    "    print(\"\\nTo profile kernels locally:\")\n",
    "    print(\"  cd profiling\")\n",
    "    print(\"  ./profile.sh instance_norm\")\n",
    "    print(\"\\nThen analyze results:\")\n",
    "    print(\"  python analyze_profile.py nsight_reports/*.csv\")\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Nsight Compute Profiling\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n‚ö†Ô∏è ncu not found - profiling unavailable\")\n",
    "    print(\"\\nInstall from: https://developer.nvidia.com/nsight-compute\")\n",
    "    print(\"\\nProfiling features:\")\n",
    "    print(\"  - Kernel duration measurement\")\n",
    "    print(\"  - Memory bandwidth analysis\")\n",
    "    print(\"  - GPU utilization tracking\")\n",
    "    print(\"  - Warp occupancy analysis\")\n",
    "    print(\"  - Optimization recommendations\")\n",
    "\n",
    "# Show profiling script location\n",
    "profiling_dir = Path('profiling')\n",
    "if profiling_dir.exists():\n",
    "    print(f\"\\nüìÅ Profiling directory: {profiling_dir.absolute()}\")\n",
    "    print(\"\\nFiles:\")\n",
    "    for f in profiling_dir.glob('*.py'):\n",
    "        print(f\"  - {f.name}\")\n",
    "    for f in profiling_dir.glob('*.sh'):\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Achievements\n",
    "\n",
    "### Implemented Kernels\n",
    "\n",
    "| Kernel | Speedup | Description |\n",
    "|--------|---------|-------------|\n",
    "| FusedAttention | 4-8x | Multi-head attention with QKV fusion |\n",
    "| FusedFFN | 3-5x | Feed-forward network with GELU |\n",
    "| FusedInstanceNorm2d | 2-4x | Instance normalization with affine |\n",
    "| FusedConvInstanceNormReLU | 5-8x | Conv+IN+ReLU for residual blocks |\n",
    "\n",
    "### Infrastructure\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| BenchmarkFramework | Automated timing with CUDA events |\n",
    "| BenchmarkReport | Generate MD/JSON/HTML/CSV reports |\n",
    "| BenchmarkVisualizer | Create performance charts |\n",
    "| Nsight Integration | Deep GPU profiling & analysis |\n",
    "\n",
    "### How to Use\n",
    "\n",
    "```python\n",
    "# Import kernels\n",
    "from kernels import FusedInstanceNorm2d, ResidualBlock\n",
    "\n",
    "# Use fused layer\n",
    "norm = FusedInstanceNorm2d(64).cuda()\n",
    "x = torch.randn(1, 64, 256, 256).cuda()\n",
    "y = norm(x)\n",
    "\n",
    "# Or use residual block\n",
    "block = ResidualBlock(128).cuda()\n",
    "y = block(x)\n",
    "```\n",
    "\n",
    "### Running Benchmarks\n",
    "\n",
    "```bash\n",
    "# Quick benchmark\n",
    "python run_full_benchmark.py --kernels instance_norm\n",
    "\n",
    "# Full benchmark suite\n",
    "python run_full_benchmark.py\n",
    "\n",
    "# Profile with Nsight\n",
    "cd profiling && ./profile.sh instance_norm\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
